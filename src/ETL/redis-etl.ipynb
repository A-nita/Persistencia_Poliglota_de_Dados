{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL para Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gregf\\appdata\\roaming\\python\\python38\\site-packages (from tqdm) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../dataset'\n",
    "\n",
    "USER_PATH = os.path.join(DATASET_PATH, 'users-details-2023.csv')\n",
    "CONSUMED_ANIMES_PATH = os.path.join(DATASET_PATH, 'final_animedataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para resolver um problema de versão encontrada pelo PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resolução do erro foi encontrada em uma resposta no [StackOverflow](https://stackoverflow.com/a/65010346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando inicialização do \"conector\" Neo4j spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config(\"spark.jars\", \"../../spark-redis/target/spark-redis_2.12-3.1.0-SNAPSHOT-jar-with-dependencies.jar\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo users-details-2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = spark.read.csv(USER_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_user = df_user.select(['mal ID', 'username', 'gender', 'days watched'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo final_animedataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumed_animes = spark.read.csv(CONSUMED_ANIMES_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando os agregados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando as recomendações para os usuários"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando os agregados iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m agregados \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m users_pandas \u001b[39m=\u001b[39m df_user\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[0;32m      4\u001b[0m qnt_users \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(users_pandas)\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _, user \u001b[39min\u001b[39;00m tqdm(users_pandas\u001b[39m.\u001b[39miterrows(), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(users)):\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    209\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1217\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m   1216\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mcollectToPython()\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\serializers.py:152\u001b[0m, in \u001b[0;36mFramedSerializer.load_stream\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_with_length(stream)\n\u001b[0;32m    153\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mEOFError\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\serializers.py:174\u001b[0m, in \u001b[0;36mFramedSerializer._read_with_length\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m<\u001b[39m length:\n\u001b[0;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloads(obj)\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\serializers.py:472\u001b[0m, in \u001b[0;36mCloudPickleSerializer.loads\u001b[1;34m(self, obj, encoding)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloads\u001b[39m(\u001b[39mself\u001b[39m, obj, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 472\u001b[0m     \u001b[39mreturn\u001b[39;00m cloudpickle\u001b[39m.\u001b[39;49mloads(obj, encoding\u001b[39m=\u001b[39;49mencoding)\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\types.py:2008\u001b[0m, in \u001b[0;36m_create_row_inbound_converter.<locals>.<lambda>\u001b[1;34m(*a)\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_row_inbound_converter\u001b[39m(dataType: DataType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Callable:\n\u001b[1;32m-> 2008\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39ma: dataType\u001b[39m.\u001b[39;49mfromInternal(a)\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\types.py:1019\u001b[0m, in \u001b[0;36mStructType.fromInternal\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1016\u001b[0m values: Union[Tuple, List]\n\u001b[0;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needSerializeAnyField:\n\u001b[0;32m   1018\u001b[0m     \u001b[39m# Only calling fromInternal function for fields that need conversion\u001b[39;00m\n\u001b[1;32m-> 1019\u001b[0m     values \u001b[39m=\u001b[39m [\n\u001b[0;32m   1020\u001b[0m         f\u001b[39m.\u001b[39mfromInternal(v) \u001b[39mif\u001b[39;00m c \u001b[39melse\u001b[39;00m v\n\u001b[0;32m   1021\u001b[0m         \u001b[39mfor\u001b[39;00m f, v, c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields, obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needConversion)\n\u001b[0;32m   1022\u001b[0m     ]\n\u001b[0;32m   1023\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m     values \u001b[39m=\u001b[39m obj\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\types.py:1020\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1016\u001b[0m values: Union[Tuple, List]\n\u001b[0;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needSerializeAnyField:\n\u001b[0;32m   1018\u001b[0m     \u001b[39m# Only calling fromInternal function for fields that need conversion\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m     values \u001b[39m=\u001b[39m [\n\u001b[1;32m-> 1020\u001b[0m         f\u001b[39m.\u001b[39;49mfromInternal(v) \u001b[39mif\u001b[39;00m c \u001b[39melse\u001b[39;00m v\n\u001b[0;32m   1021\u001b[0m         \u001b[39mfor\u001b[39;00m f, v, c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfields, obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needConversion)\n\u001b[0;32m   1022\u001b[0m     ]\n\u001b[0;32m   1023\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m     values \u001b[39m=\u001b[39m obj\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\types.py:668\u001b[0m, in \u001b[0;36mStructField.fromInternal\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfromInternal\u001b[39m(\u001b[39mself\u001b[39m, obj: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 668\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataType\u001b[39m.\u001b[39;49mfromInternal(obj)\n",
      "File \u001b[1;32mc:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\types.py:280\u001b[0m, in \u001b[0;36mTimestampType.fromInternal\u001b[1;34m(self, ts)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfromInternal\u001b[39m(\u001b[39mself\u001b[39m, ts: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m datetime\u001b[39m.\u001b[39mdatetime:\n\u001b[0;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m ts \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         \u001b[39m# using int to avoid precision loss in float\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m         \u001b[39mreturn\u001b[39;00m datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mfromtimestamp(ts \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m1000000\u001b[39;49m)\u001b[39m.\u001b[39mreplace(microsecond\u001b[39m=\u001b[39mts \u001b[39m%\u001b[39m \u001b[39m1000000\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "agregados = {}\n",
    "\n",
    "users_pandas = df_user.toPandas()\n",
    "qnt_users = len(users_pandas)\n",
    "\n",
    "for _, user in tqdm(users_pandas.iterrows(), total=len(users)):\n",
    "    user = users[i]\n",
    "    user_id = user['mal ID']\n",
    "    \n",
    "    agregados[user_id] = {}\n",
    "    agregados[user_id]['username'] = user['username']\n",
    "    agregados[user_id]['gender'] = user['gender']\n",
    "    agregados[user_id]['joined'] = user['joined']\n",
    "    agregados[user_id]['days watched'] = user['days watched']\n",
    "    if user_id in recomendations:\n",
    "        agregados[user_id]['recomendations'] = recomendations[user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando aos agregados os animes que os usuários assistiram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformando agregados de dicionário para DataFrame Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os dados no Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
