{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL para Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gregf\\appdata\\roaming\\python\\python38\\site-packages (from tqdm) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (from pyarrow) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from neo4j_consulta import generate_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../dataset'\n",
    "\n",
    "USER_PATH = os.path.join(DATASET_PATH, 'users-details-2023.csv')\n",
    "USER_FILTERED_PATH = os.path.join(DATASET_PATH, 'user-filtered.csv')\n",
    "ANIME_FILTERED_PATH = os.path.join(DATASET_PATH, 'anime-filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para resolver um problema de versão encontrada pelo PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resolução do erro foi encontrada em uma resposta no [StackOverflow](https://stackoverflow.com/a/65010346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando inicialização do \"conector\" Neo4j spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config(\"spark.jars\", \"../../spark-redis/target/spark-redis_2.12-3.1.0-SNAPSHOT-jar-with-dependencies.jar\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo users-details-2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = spark.read.csv(USER_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_user = df_user.select(['mal ID', 'username', 'gender', 'days watched', 'completed', 'dropped', 'location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo anime-filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime_filtered = spark.read.csv(ANIME_FILTERED_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_anime_filtered = df_anime_filtered.select(['anime_id', 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo user_filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_filtered = spark.read.csv(USER_FILTERED_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_user_filtered = df_user_filtered.select(['anime_id', 'user_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo join entre anime-filtered.csv e user_filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumed_animes = df_user_filtered.join(df_anime_filtered, 'anime_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando os agregados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando as recomendações para os usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = generate_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1735, 'name': 'Naruto: Shippuuden'}, {'id': 1535, 'name': 'Death Note'}, {'id': 5114, 'name': 'Fullmetal Alchemist: Brotherhood'}]\n"
     ]
    }
   ],
   "source": [
    "print(recommendations[148])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando os agregados iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731290/731290 [00:11<00:00, 62442.57it/s] \n"
     ]
    }
   ],
   "source": [
    "agregados = {}\n",
    "\n",
    "users = df_user.collect()\n",
    "qnt_users = len(users)\n",
    "\n",
    "for user in tqdm(users, total=qnt_users):\n",
    "    user_id = user['mal ID']\n",
    "    \n",
    "    agregados[user_id] = {}\n",
    "    agregados[user_id]['username'] = user['username']\n",
    "    agregados[user_id]['gender'] = user['gender']\n",
    "    agregados[user_id]['days watched'] = user['days watched']\n",
    "    agregados[user_id]['completed'] = user['completed']\n",
    "    agregados[user_id]['dropped'] = user['dropped']\n",
    "    agregados[user_id]['location'] = user['location']\n",
    "    \n",
    "    if user_id in recommendations:\n",
    "        agregados[user_id]['recomendations'] = recommendations[user_id]\n",
    "    \n",
    "    agregados[user_id]['watched animes'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'Xinil',\n",
       " 'gender': 'Male',\n",
       " 'days watched': 142.3,\n",
       " 'completed': 233.0,\n",
       " 'dropped': 93.0,\n",
       " 'location': 'California',\n",
       " 'recomendations': [{'id': 5114, 'name': 'Fullmetal Alchemist: Brotherhood'},\n",
       "  {'id': 9253, 'name': 'Steins;Gate'},\n",
       "  {'id': 2904, 'name': 'Code Geass: Hangyaku no Lelouch R2'}],\n",
       " 'watched animes': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agregados[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando os ultimos animes assistidos ao usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@pandas_udf(ArrayType(\n",
    "                StructType([\n",
    "                    StructField('id', IntegerType()),\n",
    "                    StructField('name', StringType()),\n",
    "                    StructField('img', StringType())\n",
    "])), PandasUDFType.GROUPED_AGG)  \n",
    "def last_3_udf(anime_ids, anime_names):\n",
    "    last_animes = []\n",
    "    for anime_id, anime_name in zip(anime_ids[-3:], anime_names[-3:]):\n",
    "        last_animes.append({\n",
    "            'id': anime_id,\n",
    "            'name': anime_name,\n",
    "            'img': 'img-path'\n",
    "        })\n",
    "    return last_animes\n",
    "\n",
    "results = df_consumed_animes.groupby('user_id').agg(last_3_udf(df_consumed_animes.anime_id, df_consumed_animes.name)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    if result['user_id'] in agregados:\n",
    "        agregados[result['user_id']]['watched animes'] = []\n",
    "        for row in result['min_udf(anime_id, name)']:\n",
    "            agregados[result['user_id']]['watched animes'].append({\n",
    "                'id': row['id'],\n",
    "                'name': row['name'],\n",
    "                'img': row['img']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'Aokaado',\n",
       " 'gender': 'Male',\n",
       " 'days watched': 68.6,\n",
       " 'completed': 137.0,\n",
       " 'dropped': 44.0,\n",
       " 'location': 'Oslo, Norway',\n",
       " 'recomendations': [{'id': 32281, 'name': 'Kimi no Na wa.'},\n",
       "  {'id': 9253, 'name': 'Steins;Gate'},\n",
       "  {'id': 28851, 'name': 'Koe no Katachi'}],\n",
       " 'watched animes': [{'id': 16524,\n",
       "   'name': 'Suisei no Gargantia',\n",
       "   'img': 'img-path'},\n",
       "  {'id': 25, 'name': 'Sunabouzu', 'img': 'img-path'},\n",
       "  {'id': 759, 'name': 'Tokyo Godfathers', 'img': 'img-path'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agregados[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformando agregados de dicionário para DataFrame Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os dados no Redis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
