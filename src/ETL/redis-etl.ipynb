{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL para Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gregf\\appdata\\roaming\\python\\python38\\site-packages (from tqdm) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\gregf\\anaconda3\\envs\\pmd\\lib\\site-packages (from pyarrow) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from neo4j_consulta import generate_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../dataset'\n",
    "\n",
    "USER_PATH = os.path.join(DATASET_PATH, 'users-details-2023.csv')\n",
    "USER_FILTERED_PATH = os.path.join(DATASET_PATH, 'user-filtered.csv')\n",
    "ANIME_FILTERED_PATH = os.path.join(DATASET_PATH, 'anime-filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para resolver um problema de versão encontrada pelo PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resolução do erro foi encontrada em uma resposta no [StackOverflow](https://stackoverflow.com/a/65010346)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando inicialização do \"conector\" Neo4j spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config(\"spark.jars\", \"../../spark-redis/target/spark-redis_2.12-3.1.0-SNAPSHOT-jar-with-dependencies.jar\").\\\n",
    "        config('spark.executor.memory', '8g').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo users-details-2023.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = spark.read.csv(USER_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_user = df_user.select(['mal ID', 'username', 'gender', 'days watched', 'completed', 'dropped', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mal ID', 'int'),\n",
       " ('username', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('days watched', 'double'),\n",
       " ('completed', 'double'),\n",
       " ('dropped', 'double'),\n",
       " ('location', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df_user.withColumn(\"completed\", col(\"completed\").cast(IntegerType()))\\\n",
    "                 .withColumn(\"dropped\", col(\"dropped\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo anime-filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime_filtered = spark.read.csv(ANIME_FILTERED_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_anime_filtered = df_anime_filtered.select(['anime_id', 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo o arquivo user_filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_filtered = spark.read.csv(USER_FILTERED_PATH, inferSchema=True, header=True, escape='\"', multiLine=True)\n",
    "df_user_filtered = df_user_filtered.select(['anime_id', 'user_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo join entre anime-filtered.csv e user_filtered.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumed_animes = df_user_filtered.join(df_anime_filtered, 'anime_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumed_animes = df_consumed_animes.withColumn(\"img\", lit('img-path'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando os agregados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coletando as recomendações para os usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = generate_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1735, 'name': 'Naruto: Shippuuden', 'img': 'img-path'}, {'id': 1535, 'name': 'Death Note', 'img': 'img-path'}, {'id': 5114, 'name': 'Fullmetal Alchemist: Brotherhood', 'img': 'img-path'}]\n"
     ]
    }
   ],
   "source": [
    "print(recommendations[148])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando os agregados iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731290/731290 [00:10<00:00, 68762.40it/s] \n"
     ]
    }
   ],
   "source": [
    "agregados = {}\n",
    "\n",
    "users = df_user.collect()\n",
    "qnt_users = len(users)\n",
    "\n",
    "for user in users:\n",
    "    user_id = user['mal ID']\n",
    "    \n",
    "    agregados[user_id] = {}\n",
    "    agregados[user_id]['username'] = user['username']\n",
    "    agregados[user_id]['gender'] = user['gender']\n",
    "    agregados[user_id]['days watched'] = user['days watched']\n",
    "    agregados[user_id]['completed'] = user['completed']\n",
    "    agregados[user_id]['dropped'] = user['dropped']\n",
    "    agregados[user_id]['location'] = user['location']\n",
    "    \n",
    "    if user_id in recommendations:\n",
    "        agregados[user_id]['recomendations'] = recommendations[user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando os ultimos animes assistidos ao usuários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gregf\\anaconda3\\envs\\PMD\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "anime_list_schema = ArrayType(\n",
    "                        StructType([\n",
    "                            StructField('id', IntegerType()),\n",
    "                            StructField('name', StringType()),\n",
    "                            StructField('img', StringType())\n",
    "                    ]))\n",
    "\n",
    "@pandas_udf(anime_list_schema, PandasUDFType.GROUPED_AGG)  \n",
    "def last_3_udf(anime_ids, anime_names, anime_imgs):\n",
    "    last_animes = []\n",
    "    for anime_id, anime_name, anime_img in zip(anime_ids[-3:], anime_names[-3:], anime_imgs[-3:]):\n",
    "        last_animes.append({\n",
    "            'id': anime_id,\n",
    "            'name': anime_name,\n",
    "            'img': anime_img\n",
    "        })\n",
    "    return last_animes\n",
    "\n",
    "results = df_consumed_animes.groupby('user_id').agg(last_3_udf(df_consumed_animes.anime_id, df_consumed_animes.name)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    if result['user_id'] in agregados:\n",
    "        agregados[result['user_id']]['watched animes'] = []\n",
    "        for row in result['last_3_udf(anime_id, name)']:\n",
    "            agregados[result['user_id']]['watched animes'].append({\n",
    "                'id': row['id'],\n",
    "                'name': row['name'],\n",
    "                'img': row['img']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'Xinil',\n",
       " 'gender': 'Male',\n",
       " 'days watched': 142.3,\n",
       " 'completed': 233,\n",
       " 'dropped': 93,\n",
       " 'location': 'California',\n",
       " 'recomendations': [{'id': 5114,\n",
       "   'name': 'Fullmetal Alchemist: Brotherhood',\n",
       "   'img': 'img-path'},\n",
       "  {'id': 9253, 'name': 'Steins;Gate', 'img': 'img-path'},\n",
       "  {'id': 2904,\n",
       "   'name': 'Code Geass: Hangyaku no Lelouch R2',\n",
       "   'img': 'img-path'}],\n",
       " 'watched animes': [{'id': 3588, 'name': 'Soul Eater', 'img': 'img-path'},\n",
       "  {'id': 41487,\n",
       "   'name': 'Tensei shitara Slime Datta Ken 2nd Season Part 2',\n",
       "   'img': 'img-path'},\n",
       "  {'id': 41488,\n",
       "   'name': 'Tensura Nikki: Tensei shitara Slime Datta Ken',\n",
       "   'img': 'img-path'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agregados[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranformando agregados de dicionário para DataFrame Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_user_df = spark.createDataFrame([\n",
    "    {\n",
    "        'id': user_id,\n",
    "        'documento': json.dumps(values)\n",
    "    } \n",
    "    for user_id, values in agregados.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|           documento| id|\n",
      "+--------------------+---+\n",
      "|{\"username\": \"Xin...|  1|\n",
      "|{\"username\": \"Aok...|  3|\n",
      "|{\"username\": \"Cry...|  4|\n",
      "|{\"username\": \"Arc...|  9|\n",
      "|{\"username\": \"Mad...| 18|\n",
      "|{\"username\": \"von...| 20|\n",
      "|{\"username\": \"Amu...| 23|\n",
      "|{\"username\": \"Bam...| 36|\n",
      "|{\"username\": \"bed...| 44|\n",
      "|{\"username\": \"kei...| 47|\n",
      "|{\"username\": \"Lad...| 53|\n",
      "|{\"username\": \"Hir...| 66|\n",
      "|{\"username\": \"Cru...| 70|\n",
      "|{\"username\": \"Smo...| 71|\n",
      "|{\"username\": \"Emp...| 77|\n",
      "|{\"username\": \"koa...| 80|\n",
      "|{\"username\": \"Ach...| 82|\n",
      "|{\"username\": \"jaa...| 83|\n",
      "|{\"username\": \"xic...| 90|\n",
      "|{\"username\": \"mar...| 91|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('documento', 'string'), ('id', 'bigint')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_user_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando os dados no Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_user_df.write\\\n",
    "    .format(\"org.apache.spark.sql.redis\")\\\n",
    "    .option(\"table\", \"users\")\\\n",
    "    .option(\"key.column\", \"id\")\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
